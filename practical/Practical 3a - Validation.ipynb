{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 3 - Validation\n",
    "****\n",
    "In this practical session you will explore how to validate the prediction models developed in practical 1.\n",
    "\n",
    "As shown earlier, we can use the 'predict()' function in R to estimate predicted risks from our newly developed CPM.\n",
    "\n",
    "We will then outline the process of how to use such predicted risks to validate the prediction model. As discussed in the lecture notes, this should be undertaken in new (independent) data. However, we do not have external data to validate our model. For purposes of illustration of the R code, we here test performance within the development dataset. We will call this the 'apparent performance' of the model. Both the calibration and discrimination of the model will be calculated.\n",
    "\n",
    "It is often the case that we do not have external data, but dont worry there is no need to panic or split our data. We can perform internal validation. The latter part of this practical will illustrate how to perform bootstrap internal validation.\n",
    "\n",
    "We will focus only on the model developed in practical 1. The majority of the validation techniques and code still hold for the survival models by evaluating at one or more timepoints. Where there are differences between the logistic and survival valdation, they are discused in the text. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets load the packages we will be using in the practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(pROC)\n",
    "library(DescTools)\n",
    "library(Hmisc)\n",
    "library(plyr)\n",
    "library(boot)\n",
    "library(MASS)\n",
    "library(rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by loading the data and subset, as we have done in previous practicals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart <- readRDS(\"SMARTs_P2-3.rds\")\n",
    "smart <- subset(smart, select = c(outcome, TEVENT, EVENT, SEX, AGE, SBP, alcohol, CHOLO, BMIO, DIABETES, CARDIAC, SMOKING, AAA))\n",
    "smart <- na.omit(smart)\n",
    "attach(smart)\n",
    "class(smart)\n",
    "sapply(smart,class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the multivariable models selected from practical 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age3_spline <- rcs(AGE,3)\n",
    "\n",
    "# Fit model selected from practicals 1 and 2\n",
    "mod_log <- glm(outcome~SEX+SBP+DIABETES+age3_spline,family=\"binomial\",x=TRUE,y=TRUE)\n",
    "mod_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 apparent discrimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrimination refers to the ability of a CPM to separate patients who will develop/experiance an outcome from those who will not.  \n",
    "The C-statistic (concordance) is typically used to assess model discrimination. This is calculated as the concordant pairs of individuals. If we take two individuals, one who experienced the outcome and one who did not, and the individual who had the event was given the higher probability, then this pair of individuals are concordant.\n",
    "We can also plot the receiver operator characteristic (ROC) curve. *Note - The area under this curve is equivalent to the C-statistic.*\n",
    "\n",
    "### Run the below code for discrimination. Can you interpret the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate apparent discrimination performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the predicted probabilities for each patient\n",
    "pred_prob <- predict(mod_log,type=\"response\")\n",
    "\n",
    "# Obtain the c statistic / AUC\n",
    "c1 <- roc(outcome~pred_prob,ci=TRUE)\n",
    "c1\n",
    "\n",
    "# Plot the ROC curve\n",
    "plot(roc(outcome,pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The C-statistic can also be calculated for survival models but does not include censored individuals or individuals with the same event time. The D-statistic is often calculated instead. This was proposed by Royston and Sauerbrei and their paper can be found [here](https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.1621).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 apparent calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets begin by calculating calibration-in-the-large (CITL). This compares the mean of all predicted risks with the mean observed risk. This is often used to indicate the extent that predictions are systematically too low or too high. \n",
    "If CITL > 0, observed proportions are higher than the predicted probabilities.  \n",
    "If CITL < 0, predicted probabilities are higher than the observed proportions.\n",
    "\n",
    "We can also use the expected/observed ratio (E/O) by dividing the expected by the observed number of events (or probabilities).  \n",
    "E/O is related to CITL and when E/O<1, CITL>0. Also, when E/O>1, CITL<0  \n",
    "*Note - When overall prevelence is low, E/O as a ratio can look very large even when differences are smalll.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the linear predictor for each patient\n",
    "pred_LP <- predict(mod_log,type=\"link\")\n",
    "\n",
    "# CITL\n",
    "mod_log_1 <- glm(outcome~offset(pred_LP),family=\"binomial\")\n",
    "coefficients(mod_log_1)\n",
    "confint(mod_log_1)\n",
    "\n",
    "# E/O\n",
    "prop.table(table(outcome))\n",
    "O <- prop.table(table(outcome))[2]\n",
    "E <- mean(pred_prob)\n",
    "E/O\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this show? Is it what you expected?why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "E/O for survival models use expected and observed event probabilities rather than the number of events. This can be calculated to one or more time points. Thereofre the expected/observed ratio at 2 years (24 months) is,  \n",
    "$$E/O(t) = \\frac{1-S_{exp}(t)}{1-S_{obs}(t)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative  way of testing calibraiton is by estimating the so-called calibration slope. This is obtained by fitting a logistic regression model to the observed outcomes and using the linear predictor of the model as the only covariate. A  calibration slope < 1 indicates the model is overfitted (too extreme risk predictions). NOTE - calibration intercept is the same as calibration in the large\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the calibration slope. Can you interpret the result and is it what you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_log_2 <- glm(outcome~pred_LP,family=\"binomial\",x=TRUE,y=TRUE)\n",
    "coefficients(mod_log_2)\n",
    "confint(mod_log_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For survival models you can obtain the the calibration slope by fitting a cox model to the LP as the only predictor.  \n",
    "For example, *coxph(surv(TEVENT, EVENT)~pred_LP,x=TRUE,y=TRUE)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to assess calibration of a CPM is a so-called calibration plot, which we make by dividing the data into several (usually ten) equally sized groups based on the ordered probabilities, and calculating the observed and expected number of events in each group. One way of doing this is using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual assessment of calibration by risk groups\n",
    "\n",
    "# create 10 risk groups\n",
    "groups <- cut(pred_prob,breaks=quantile(pred_prob, prob = c(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0)),labels=c(1:10),include.lowest=TRUE)\n",
    " \n",
    "# average the observed and expected probabilities of patients in each risk group \n",
    "gpdata <- cbind(smart,groups,pred_prob)\n",
    "obs <- (ddply(gpdata,~groups,summarise,mean=mean(as.numeric(outcome)))[,2])\n",
    "exp <- ddply(gpdata,~groups,summarise,mean=mean(pred_prob))\n",
    "obsn <- table(outcome,groups)[1,] \n",
    "\n",
    "# CIs for scatter points\n",
    "lci = pmax(0,(obs - (1.96*(((obs*(1-obs))/obsn)^.5))))\n",
    "uci = pmin(1,(obs + (1.96*(((obs*(1-obs))/obsn)^.5))))\n",
    "\n",
    "# graph a simple calibration plot over 10 risk groups\n",
    "par(pty=\"s\")\n",
    "plot(obs~exp[,2],xlim=c(0,1),ylim=c(0,1),col=\"red\",ylab=\"Observed\",xlab=\"Expected\")\n",
    "lines(c(0,1),c(0,1),lty=2)\n",
    "for(i in 1:10){\n",
    "lines(c(exp[i,2],exp[i,2]),c(lci[i],uci[i]),col=\"green\")\n",
    "}\n",
    "h <- hist(pred_prob, breaks=50, plot=FALSE)\n",
    "for(i in 1:length(h$mids)){\n",
    "  lines(c(h$mids[i],h$mids[i]),c(rep(1,length(h$mids))[i],1-((h$counts[i]/max(h$counts))/10)))\n",
    "}\n",
    "\n",
    "# Add a loess smoother to the plot\n",
    "obs_all <- predict(loess((as.numeric(outcome))~pred_prob,span=1))\n",
    "lines_data <- data.frame(pred_prob,obs_all)\n",
    "lines_data2 <- lines_data[order(pred_prob),] \n",
    "lines(lines_data2[,1],lines_data2[,2],col=\"blue\")\n",
    "legend(0.0,0.9,c(\"Risk groups\",\"Reference line\",\"95% CI\",\"Loess\"),col=c(\"red\",\"black\",\"green\",\"blue\"),lty=c(0,2,1,1),pch=c(1,NA,NA,NA),bty=\"n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "par(pty=\"s\")\n",
    "plot(obs~exp[,2],xlim=c(0,.3),ylim=c(0,.3),col=\"red\",ylab=\"Observed\",xlab=\"Expected\")\n",
    "lines(c(0,1),c(0,1),lty=2)\n",
    "for(i in 1:10){\n",
    "lines(c(exp[i,2],exp[i,2]),c(lci[i],uci[i]),col=\"green\")\n",
    "}\n",
    "h <- hist(pred_prob, breaks=50, plot=FALSE)\n",
    "for(i in 1:length(h$mids)){\n",
    "  lines(c(h$mids[i],h$mids[i]),c(rep(1,length(h$mids))[i],1-((h$counts[i]/max(h$counts))/10)))\n",
    "}\n",
    "\n",
    "# Add a loess smoother to the plot\n",
    "obs_all <- predict(loess((as.numeric(outcome))~pred_prob,span=1))\n",
    "lines_data <- data.frame(pred_prob,obs_all)\n",
    "lines_data2 <- lines_data[order(pred_prob),] \n",
    "lines(lines_data2[,1],lines_data2[,2],col=\"blue\")\n",
    "legend(0.0,0.25,c(\"Risk groups\",\"Reference line\",\"95% CI\",\"Loess\"),col=c(\"red\",\"black\",\"green\",\"blue\"),lty=c(0,2,1,1),pch=c(1,NA,NA,NA),bty=\"n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Brier score\n",
    "BrierScore(mod_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brier score is also a measure of performance and ranges from 0 to 0.25, with lower values representing better performance. The brier score of 0.051 indicates good overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of above results\n",
    "These results show that the CPM is well calibrated and has moderate discrimination. However, these performance estimates are misleading because they are based on the data for which the CPM was derived and are therefore subject to in-sample optimism.  \n",
    "\n",
    "This optimism can be adjusted for by bootstrapped validation. This is the prefered method for internal validation of a prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 bootstrap validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You can obtain c-statistic and c-slope directly from Harrell's \"validate\" programme within 'rms'.\n",
    "mod1 <- lrm(outcome~SEX+SBP+DIABETES+age3_spline,x=TRUE,y=TRUE)\n",
    "set.seed(231398)\n",
    "boot_1 <- validate(mod1,method=\"boot\",B=50)\n",
    "boot_1\n",
    "\n",
    "# Note that this gives Dxy rather than c, however Dxy = 2*(c-0.5), i.e. c=(Dxy/2)+0.5\n",
    "(boot_1[1,1]+1)/2\n",
    "(boot_1[1,5]+1)/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To get bootstrapped answers for CITL, c-statistic & c-slope it's a more manual approach:\n",
    "manual_boot <- function(data,samples){\n",
    "  results <- matrix(nrow = samples,ncol = 6)\n",
    "  set.seed(231398)\n",
    "  for (i in 1:samples) {\n",
    "    samp_index <- sample(1:nrow(data), nrow(data), rep=TRUE) # create a sampling index vector\n",
    "   \n",
    "    bs_samp <- data[samp_index,] # index the orignal dataset using the sampling vector to give the bs sample\n",
    "    model <- glm(outcome~SEX+age3_spline + DIABETES,family=binomial, data=bs_samp)  # Fit model to the bootstrap sample\n",
    "    pr_bs <- predict(model,type=\"response\") # predict probabilities from the bootstrap model in the bs sample\n",
    "    lp_bs <- predict(model) # predict lp from the bootstrap model in the bs sample\n",
    "    \n",
    "    pr_test <- predict(model,type=\"response\",newdata = data) # predict probabilities from the bootstrap model in the original sample\n",
    "    lp_test <- predict(model, newdata = data) # predict lp from the bootstrap model in the original sample\n",
    "    \n",
    "    # calculate the apparent performance of the bootstrap model in the bs sample\n",
    "    app_cstat_model <- roc(outcome~pr_bs,data=bs_samp)\n",
    "    results[i,1] <- app_cstat_model$auc\n",
    "    app_citl_model <- glm(outcome ~ offset(lp_bs),family=binomial, data=bs_samp)\n",
    "    results[i,2] <- summary(app_citl_model)$coefficients[1,1]\n",
    "    app_cslope_model <- glm(outcome ~ lp_bs,family=binomial(link='logit'), data=bs_samp)\n",
    "    results[i,3] <- summary(app_cslope_model)$coefficients[2,1]\n",
    "    \n",
    "    # calculate the test performance of the bootstrap model in the original sample\n",
    "    test_cstat_model <- roc(outcome~pr_test,data=data)\n",
    "    results[i,4] <- test_cstat_model$auc\n",
    "    test_citl_model <- glm(outcome ~ offset(lp_test),family=binomial, data=data)\n",
    "    results[i,5] <- summary(test_citl_model)$coefficients[1,1]\n",
    "    test_cslope_model <- glm(outcome ~ lp_test,family=binomial, data=data)\n",
    "    results[i,6] <- summary(test_cslope_model)$coefficients[2,1]\n",
    " \n",
    "    print(i)\n",
    "  }\n",
    "  results2 <- as.data.frame(results)\n",
    "  colnames(results2) <- c(\"app_c_stat\",\"app_citl\",\"app_c_slope\",\"test_c_stat\",\"test_citl\",\"test_c_slope\")\n",
    "  return(results2)\n",
    "}\n",
    "boot_results <- manual_boot(smart,50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimisum adjusted statistics can then be obtained.  \n",
    "We can also calculate the optimism in the validation statistics from validating in the development data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimism adjusted statistics\n",
    "c1$auc - (mean(boot_results$app_c_stat)-mean(boot_results$test_c_stat)) # c-stat\n",
    "mod_log_1$coef - (mean(boot_results$app_citl)-mean(boot_results$test_citl)) # citl\n",
    "mod_log_2$coef[2]- (mean(boot_results$app_c_slope)-mean(boot_results$test_c_slope)) # c-slope\n",
    "\n",
    "#optimism on its own\n",
    "mean(boot_results$app_c_stat)-mean(boot_results$test_c_stat)\n",
    "mean(boot_results$app_citl)-mean(boot_results$test_citl)\n",
    "mean(boot_results$app_c_slope)-mean(boot_results$test_c_slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you interpret the results? What do you conclude about the model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also provided you a CHD data set that is discussed in the intro to R file.\n",
    "\n",
    "Now you have seen how to build a model why not try and develop a logistic model for CHD.\n",
    "Note, when loading in the excel file, some variable are not imported as numeric. See \"converting a factor to numeric\" section in the intro to R. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
